{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll have a look at the transcript of the [Kamala Harris vs. Donald Trump 2024 debate](https://www.youtube.com/watch?v=lAH6KUOiSB8). \n",
    "We'll be using the data prepared by `jumonlala`, and published in the [jumonlala/harris_trump_debate](https://github.com/jumonlala/harris_trump_debate/) github repository. \n",
    "The transcript itself was sourced from [ABC News](https://abcnews.go.com/Politics/harris-trump-presidential-debate-transcript/story?id=113560542), \n",
    "and subsequently segmented and adorned with some standard NLP features (topic classification as well polarity, subjectivity, and certainty scores). \n",
    "If you're interested in the specifics of the data prep, you can find these in [this data prep notebook](https://github.com/jumonlala/harris_trump_debate/blob/main/python/Presidential_Debate_Analysis.ipynb).\n",
    "\n",
    "Specifically, we'll use the [data/harris_trump.csv](https://github.com/jumonlala/harris_trump_debate/blob/main/data/harris_trump.csv)\n",
    "data, which contains only the data for harris and trump (presenter's data filtered out).\n",
    "\n",
    "Let's jump right into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few imports we'll use\n",
    "import os\n",
    "import io\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set a local folder\n",
    "\n",
    "This is where the data will be downloaded, and artifacts (like embeddings) will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rootdir='/Users/thorwhalen/Dropbox/_odata/figiri/harris_vs_trump/'\n"
     ]
    }
   ],
   "source": [
    "rootdir = '/Users/thorwhalen/Dropbox/_odata/figiri/harris_vs_trump/'  # set your rootdir here if you want\n",
    "# ... or set it as an environment variable\n",
    "rootdir = rootdir or os.getenv('HARRIS_VS_TRUMP_DEBATE', None)\n",
    "\n",
    "# ensure rootdir exists\n",
    "if rootdir is None:\n",
    "    raise ValueError(\"rootdir is not set\")\n",
    "elif not os.path.exists(rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "print(f\"{rootdir=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dol import Files \n",
    "from graze import graze\n",
    "\n",
    "# `get_data_bytes` will download the data from a url and store it locally \n",
    "# under rootdir (using a path based on the url). \n",
    "# We use graze here, which means that the next time you call `get_data_bytes` on the same \n",
    "# url, the data will be taken from the local file instead so you're nice to the internet\n",
    "get_data_bytes = partial(graze, rootdir=rootdir)\n",
    "\n",
    "# make a local \"bytes\" store to convieniently read and write bytes to and from files under your rootdir\n",
    "local_store = Files(rootdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data (once, then get it from cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape=(1141, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                                                             14\n",
       "speaker                                             KAMALA HARRIS\n",
       "text                       So, I was raised as a middle-class kid\n",
       "topic                                                     economy\n",
       "token           ['so', 'be', 'raise', 'a', 'middle-class', 'kid']\n",
       "polarity                                                      0.0\n",
       "subjectivity                                                  0.0\n",
       "certainty                                                     1.0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_url = 'https://raw.githubusercontent.com/jumonlala/harris_trump_debate/refs/heads/main/data/harris_trump.csv'\n",
    "\n",
    "data = pd.read_csv(io.BytesIO(get_data_bytes(src_url)), index_col=0)\n",
    "print(f\"{data.shape=}\")\n",
    "\n",
    "data.iloc[0]  # have a look at the first row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have some nicely prepared data with a few features, but we want to add some more.\n",
    "Namely, we're going to vectorize (more precisely, \"get semantic vectors\" -- known nowadays as \"get embeddings\") for all text segments and also get some planar projections for these, \n",
    "so we can visualize them.\n",
    "\n",
    "As you see, the text has been tokenized and lemmatized (e.g. \"was\" -> \"be\"), and the resulting tokens are available under the `token` column. \n",
    "This is convenient for some types of analysis you may want to conduct. \n",
    "Here, though, we'll be using modern, transformer-based embeddings (specifically OpenAI's embeddings API) to visualize our text segments, and the embeddings function has it's own tokenizer; Note that this tokenizer doesn't even lemmatize these tokens explicitly -- though it probably does something like lemmatization somewhere under the hood. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get (semantic) embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now often what we'd want to feed to the embedding is the text segments list (in the order \n",
    "it appears in the data), or a {unique_key: text, ...} dictionary, so as to be able to \n",
    "align/join our embedding vectors back to the data. \n",
    "As the data you're dealing with grows though, you'll find that this way can hit some difficulties. \n",
    "\n",
    "If you're referencing you data with a key, you better make sure that your key is unique. \n",
    "In our case, it seems like the `id` column would be the candidate for referencing the rows:\n",
    "Indeed I almost did that myself, and then I checked and found out the `id` as 96 unique values\n",
    "(so the id is probably some vestigial artifact from data prep). \n",
    "A better choice would be the index of the dataframe itself. \n",
    "\n",
    "If you use the text segments list itself, in order, you're essentially using the list's (unique)\n",
    "index as your \"alignment information\", but you could be wasting time and money if many segments are duplicated\n",
    "(which can happen a lot in speech). Sure, embeddings are cheap, but in some cases it can add up!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ids: 96\n",
      "Number of text segments: 1141\n",
      "Number of unique text segments: 1132\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique ids: {data['id'].nunique()}\")\n",
    "print(f\"Number of text segments: {len(data)}\")\n",
    "print(f\"Number of unique text segments: {data['text'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, go on! Let's see what these duplicated text segments are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated text segments: 18\n",
      "Number of unique duplicated text segments: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>DONALD TRUMP</td>\n",
       "      <td>He's got nuclear weapons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>DONALD TRUMP</td>\n",
       "      <td>He's got nuclear weapons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>DONALD TRUMP</td>\n",
       "      <td>I don't know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>DONALD TRUMP</td>\n",
       "      <td>I don't know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>KAMALA HARRIS</td>\n",
       "      <td>Let's not go back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>KAMALA HARRIS</td>\n",
       "      <td>Let's not go back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>DONALD TRUMP</td>\n",
       "      <td>She hates Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>DONALD TRUMP</td>\n",
       "      <td>She hates Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>DONALD TRUMP</td>\n",
       "      <td>She is Biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>DONALD TRUMP</td>\n",
       "      <td>She is Biden</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           speaker                      text\n",
       "861   DONALD TRUMP  He's got nuclear weapons\n",
       "859   DONALD TRUMP  He's got nuclear weapons\n",
       "938   DONALD TRUMP              I don't know\n",
       "81    DONALD TRUMP              I don't know\n",
       "616  KAMALA HARRIS         Let's not go back\n",
       "628  KAMALA HARRIS         Let's not go back\n",
       "757   DONALD TRUMP          She hates Israel\n",
       "753   DONALD TRUMP          She hates Israel\n",
       "974   DONALD TRUMP              She is Biden\n",
       "971   DONALD TRUMP              She is Biden"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_text = data[data.duplicated('text', keep=False)]\n",
    "print(f\"Number of duplicated text segments: {len(duplicated_text)}\")\n",
    "print(f\"Number of unique duplicated text segments: {duplicated_text['text'].nunique()}\")\n",
    "duplicated_text[['speaker', 'text']].sort_values('text').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dol import written_bytes, read_from_bytes  # tools to change file-io to byte-io functions \n",
    "\n",
    "df_to_parquet_bytes = written_bytes(pd.DataFrame.to_parquet)\n",
    "\n",
    "save_filename = 'openai_embeddings.parquet'\n",
    "\n",
    "if save_filename in local_store:\n",
    "    # get the saved embeddings from the local store\n",
    "    bytes_ = local_store[save_filename]\n",
    "    text_and_embeddings_df = read_from_bytes(pd.read_parquet, bytes_)\n",
    "else:\n",
    "    print(\"Computing (then saving) OpenAI embeddings for the text segments...\")\n",
    "    # compute and save embeddings (this took 6.5s on my computer)\n",
    "    import oa  # pip install oa  (you'll also need to set OPENAI_API_KEY as an environment variable)\n",
    "\n",
    "    # note: the sorted is important to ensure you'll be able to have a consistent order \n",
    "    # and be able to align the embeddings with the text\n",
    "    sorted_unique_text_segments = sorted(set(data['text'])) \n",
    "\n",
    "    embeddings = oa.embeddings(sorted_unique_text_segments)\n",
    "    text_and_embeddings_df = pd.DataFrame({'text': sorted_unique_text_segments, 'embeddings': embeddings})\n",
    "    local_store[save_filename] = df_to_parquet_bytes(text_and_embeddings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_and_embeddings_df.shape=(1132, 2)\n",
      "\n",
      "First row: text          $6,000 for young families for the first year o...\n",
      "embeddings    [0.018508581444621086, 0.006474844645708799, 0...\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Size of vectors: 1536\n"
     ]
    }
   ],
   "source": [
    "print(f\"{text_and_embeddings_df.shape=}\\n\")\n",
    "print(f\"First row: {text_and_embeddings_df.iloc[0]}\\n\")\n",
    "print(f\"Size of vectors: {len(text_and_embeddings_df['embeddings'].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting planar embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you want to visualize these multi-dimensional points on a screen. \n",
    "This is going to require (implicitly or explicitly) to get some planar (that is, `(x, y)`) coordinates for each of your points. \n",
    "There's many ways to do this. How do you chose. \n",
    "\n",
    "Well, first: **Why are you doing this?**.\n",
    "\n",
    "You’re not just passively observing your data; you’re courting it, trying to tease out the stories it’s holding back. Choosing how to slice it—how to look at it—becomes an act of intent, of will, even of ideology. The projection you pick shapes the truth you end up seeing. Sometimes you’re playing favorites, picking the lens that aligns with your narrative. Other times, you’re a relentless skeptic, rotating through different projections, mining for the subtle cracks where deeper truths might leak through. Each choice, each transformation, tells a different story, frames a different reality—and understanding this is half the game.\n",
    "\n",
    "Here, we'll try a few representative methods, sticking to those that are included in scikit-learn, which pretty much everyone has lying around. \n",
    "But know that there are many more choices to check out. Namely, \n",
    "[Uniform Manifold Approximation and Projection (UMAP)](https://umap-learn.readthedocs.io/en/latest/) (see [umap-learn](https://umap-learn.readthedocs.io/en/latest/) package),\n",
    "[Autoencoders](https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/) (see [TensorFlow](https://www.tensorflow.org/guide/keras/autoencoders) or [PyTorch](https://pytorch.org/tutorials/recipes/recipes/autoencoder_recipe.html) packages),\n",
    "[Projection Pursuit](https://en.wikipedia.org/wiki/Projection_pursuit) (see [direpack](https://pypi.org/project/direpack/) package), \n",
    "[Independent Component Analysis (ICA)](https://cmdlinetips.com/2022/08/7-dimensionality-reduction-techniques-by-examples-in-python/) (see [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html) package),\n",
    "[Maximum Variance Unfolding (MVU)](https://en.wikipedia.org/wiki/Semidefinite_embedding) (see [MVU](https://github.com/lvdmaaten/mvu) package), etc. \n",
    "\n",
    "| **Method Name**          | **What it Reveals**                                  | **Advantages**                                                                 | **Disadvantages**                                                           | **scikit-learn Code**                                                                                          |\n",
    "|---------------------------|-----------------------------------------------------|-------------------------------------------------------------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| **PCA**                  | Dominant variance patterns in data.                 | Simple, interpretable, and computationally efficient.                        | Only linear relationships; ignores local or nonlinear structures.           | `from sklearn.decomposition import PCA`<br>`pca = PCA(n_components=2)`<br>`planar_embeddings = pca.fit_transform(X)`       |\n",
    "| **Random Projection**     | Preserves pairwise distances approximately.         | Extremely fast and lightweight; good for large datasets.                     | Lack of interpretability; may lose significant structure.                   | `from sklearn.random_projection import GaussianRandomProjection`<br>`rp = GaussianRandomProjection(n_components=2)`<br>`planar_embeddings = rp.fit_transform(X)` |\n",
    "| **t-SNE**                | Local clustering and similarity relationships.       | Captures nonlinear local structure; visually intuitive.                      | Computationally expensive; sensitive to hyperparameters.                    | `from sklearn.manifold import TSNE`<br>`tsne = TSNE(n_components=2, random_state=42)`<br>`planar_embeddings = tsne.fit_transform(X)` |\n",
    "| **Kernel PCA**            | Nonlinear patterns using a kernel function.         | Captures nonlinear relationships; flexible with choice of kernels.           | Computationally intensive; kernel selection can be tricky.                  | `from sklearn.decomposition import KernelPCA`<br>`kpca = KernelPCA(n_components=2, kernel='rbf')`<br>`planar_embeddings = kpca.fit_transform(X)` |\n",
    "| **Linear Discriminant Analysis (LDA)** | Maximizes separability between categories.           | Directly focuses on class separability; interpretable.                       | Assumes linear boundaries; sensitive to imbalanced classes.                 | `from sklearn.discriminant_analysis import LinearDiscriminantAnalysis`<br>`lda = LinearDiscriminantAnalysis(n_components=2)`<br>`planar_embeddings = lda.fit_transform(X, y)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate the lists of embeddings into a 2D array\n",
    "X = np.stack(text_and_embeddings_df['embeddings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pca...\n",
      "Fitting random_projection...\n",
      "Fitting tsne...\n",
      "Fitting kernel_pca...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.manifold import TSNE, SpectralEmbedding\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "planar_embedding_unsupervised_learners = {\n",
    "    'pca': PCA(n_components=2),\n",
    "    'random_projection': GaussianRandomProjection(n_components=2, random_state=42),\n",
    "    'tsne': TSNE(n_components=2, random_state=42),\n",
    "    'kernel_pca': KernelPCA(n_components=2, kernel='rbf'),\n",
    "}\n",
    "\n",
    "# fit the learners\n",
    "for name, learner in planar_embedding_unsupervised_learners.items():\n",
    "\n",
    "    print(f\"Fitting {name}...\")\n",
    "    X_2d = learner.fit_transform(X)\n",
    "    text_and_embeddings_df[f'{name}__x'] = X_2d[:, 0]\n",
    "    text_and_embeddings_df[f'{name}__y'] = X_2d[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_store['harris_vs_trump_debate.parquet'] = df_to_parquet_bytes(text_and_embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index'] = range(10, len(df)+ 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         10\n",
       "1         11\n",
       "2         12\n",
       "3         13\n",
       "4         14\n",
       "        ... \n",
       "1136    1146\n",
       "1137    1147\n",
       "1138    1148\n",
       "1139    1149\n",
       "1140    1150\n",
       "Name: index, Length: 1141, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=1141, step=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                     14\n",
       "speaker                                                     KAMALA HARRIS\n",
       "text                               So, I was raised as a middle-class kid\n",
       "topic                                                             economy\n",
       "token                   ['so', 'be', 'raise', 'a', 'middle-class', 'kid']\n",
       "polarity                                                              0.0\n",
       "subjectivity                                                          0.0\n",
       "certainty                                                             1.0\n",
       "embeddings              [0.015647249296307564, -0.006923337001353502, ...\n",
       "pca__x                                                          -0.143514\n",
       "pca__y                                                           0.135135\n",
       "random_projection__x                                            -1.130084\n",
       "random_projection__y                                              1.32939\n",
       "tsne__x                                                        -36.573841\n",
       "tsne__y                                                         16.835093\n",
       "kernel_pca__x                                                   -0.005177\n",
       "kernel_pca__y                                                   -0.004871\n",
       "_index                                                                  0\n",
       "speaker_id                                                              0\n",
       "speaker_color                                                        blue\n",
       "topic_id                                                                0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "\n",
    "text_and_embeddings_df = pd.read_parquet(io.BytesIO(local_store['harris_vs_trump_debate.parquet']))\n",
    "\n",
    "\n",
    "# join data to text_and_embeddings_df\n",
    "df = data.merge(text_and_embeddings_df, on='text', how='left')\n",
    "\n",
    "df['_index'] = range(len(df))  # TODO: get rid of need of this\n",
    "\n",
    "# make a column that has a unique id for each speaker\n",
    "df['speaker_id'] = df['speaker'].map({speaker: i for i, speaker in enumerate(df['speaker'].unique())})\n",
    "colors = ['blue', 'red']\n",
    "df['speaker_color'] = df['speaker'].map({speaker: colors[i] for i, speaker in enumerate(df['speaker'].unique())})\n",
    "# same for topic\n",
    "df['topic_id'] = df['topic'].map({topic: i for i, topic in enumerate(df['topic'].unique())})\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28be18816d2b433f9cebd2b191b92524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, default_link_color=None, default_point_color=None, focused_point_ring_color=…"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cosmograph_widget import Cosmograph\n",
    "\n",
    "Cosmograph(\n",
    "   points=df, \n",
    "   \n",
    "   point_id='_index',\n",
    "   point_index='_index',\n",
    "\n",
    "   point_color='speaker_color',\n",
    "   point_x='polarity',\n",
    "   point_y='certainty',\n",
    "   point_size_scale=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmograph._resources import _get_fresh_color_table, ResourcesDacc\n",
    "\n",
    "resouces = ResourcesDacc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_html('https://www.w3.org/TR/css-color-4/#named-colors')\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color_name</th>\n",
       "      <th>hex_rgb</th>\n",
       "      <th>decimal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aliceblue</td>\n",
       "      <td>#f0f8ff</td>\n",
       "      <td>240 248 255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>antiquewhite</td>\n",
       "      <td>#faebd7</td>\n",
       "      <td>250 235 215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aqua</td>\n",
       "      <td>#00ffff</td>\n",
       "      <td>0 255 255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aquamarine</td>\n",
       "      <td>#7fffd4</td>\n",
       "      <td>127 255 212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>azure</td>\n",
       "      <td>#f0ffff</td>\n",
       "      <td>240 255 255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>wheat</td>\n",
       "      <td>#f5deb3</td>\n",
       "      <td>245 222 179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>white</td>\n",
       "      <td>#ffffff</td>\n",
       "      <td>255 255 255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>whitesmoke</td>\n",
       "      <td>#f5f5f5</td>\n",
       "      <td>245 245 245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>yellow</td>\n",
       "      <td>#ffff00</td>\n",
       "      <td>255 255 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>yellowgreen</td>\n",
       "      <td>#9acd32</td>\n",
       "      <td>154 205 50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       color_name  hex_rgb      decimal\n",
       "0       aliceblue  #f0f8ff  240 248 255\n",
       "1    antiquewhite  #faebd7  250 235 215\n",
       "2            aqua  #00ffff    0 255 255\n",
       "3      aquamarine  #7fffd4  127 255 212\n",
       "4           azure  #f0ffff  240 255 255\n",
       "..            ...      ...          ...\n",
       "143         wheat  #f5deb3  245 222 179\n",
       "144         white  #ffffff  255 255 255\n",
       "145    whitesmoke  #f5f5f5  245 245 245\n",
       "146        yellow  #ffff00    255 255 0\n",
       "147   yellowgreen  #9acd32   154 205 50\n",
       "\n",
       "[148 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_color_table_columns = ['Color\\xa0name', 'Hex\\xa0rgb', 'Decimal']\n",
    "color_table = next(filter(lambda df: set(_color_table_columns).issubset(df.columns), t), None)\n",
    "color_table = color_table[_color_table_columns]\n",
    "color_table.columns = color_table.columns.map(lambda x: x.replace('\\xa0', '_').lower())\n",
    "assert color_table['color_name'].nunique() == len(color_table), \"color names are not unique\"\n",
    "color_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/thorwhalen/Dropbox/py/proj/c/cosmograph_widget/src/cosmograph_widget/__init__.py'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cosmograph_widget\n",
    "\n",
    "cosmograph_widget.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://c.contentsquare.net/custom-errors?v=15.36.2&pid=1973&pn=61&sn=2&uu=5e8e991a-0bf8-a86f-ccf2-3f2999e75d46&ct=0\n",
    "https://k-aeu1.contentsquare.net/v2/recording?rt=7&rst=1733389596145&let=1733389670624&v=15.36.2&pid=1973&pn=61&sn=2&uu=5e8e991a-0bf8-a86f-ccf2-3f2999e75d46&ri=6&ct=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cosmograph_widget import Cosmograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=2).fit(X, data['speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape=(1141, 8)\n",
      "\n",
      "First row:\n",
      "id                                                             14\n",
      "speaker                                             KAMALA HARRIS\n",
      "text                       So, I was raised as a middle-class kid\n",
      "topic                                                     economy\n",
      "token           ['so', 'be', 'raise', 'a', 'middle-class', 'kid']\n",
      "polarity                                                      0.0\n",
      "subjectivity                                                  0.0\n",
      "certainty                                                     1.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"{data.shape=}\\n\")\n",
    "print(f\"First row:\\n{data.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

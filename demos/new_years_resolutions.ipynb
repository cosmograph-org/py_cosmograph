{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Years Resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so, here we are again. The calendar flips, and suddenly it’s January—time for that strange, almost ritualistic exercise in self-reinvention we call New Year’s resolutions. You know the drill: the vaguely nauseating societal expectation that you will, at this arbitrary cosmic checkpoint, decide to overhaul your life. Or at least pretend to, so the well-meaning but oppressively curious questioners (“So, any resolutions this year?”) can have their moment of mild but undeniable judgment while you scramble to articulate something—anything—that sounds simultaneously profound and achievable, despite knowing deep down that life is already…well, fine. Not perfect, but fine.\n",
    "\n",
    "But what if, instead of passively dreading the resolution inquisition, you armed yourself? Not with actual resolutions, per se—because let’s be honest, who has the energy for actual follow-through?—but with a set of wry, irreverent, possibly brilliant ideas drawn from the great chaotic hive mind of the internet. Specifically, from nearly 5,000 New Year’s resolution tweets, harvested and distilled for your convenience. Not so much resolutions as ammunition. A way to deflect, charm, and outwit your interrogators, leaving them dazzled and perhaps even envious of your meta-level resolution game.\n",
    "\n",
    "Ready? Let’s dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4723, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_created                                       2014-12-21 16:11:00\n",
       "tweet_text            #NewYearsResolution to not put the parking lot...\n",
       "tweet_category                                                    Humor\n",
       "tweet_topics          Humor about Personal Growth and Interests Reso...\n",
       "tweet_location                                       City of Angels, CA\n",
       "tweet_state                                                          CA\n",
       "tweet_region                                                       West\n",
       "user_timezone                                Pacific Time (US & Canada)\n",
       "user_gender                                                        male\n",
       "retweet_count                                                       NaN\n",
       "text                  to not put the parking lot ticket directly in ...\n",
       "topics_cluster_7                                                      2\n",
       "_id                                                                   0\n",
       "pca_x                                                          0.082814\n",
       "pca_y                                                           0.23159\n",
       "tsne_x                                                        16.665836\n",
       "tsne_y                                                       -50.568066\n",
       "umap_x                                                         0.597796\n",
       "umap_y                                                         0.925406\n",
       "tweet_topics_x                                                -0.453267\n",
       "tweet_topics_y                                                 1.574798\n",
       "tweet_state_x                                                  1.208174\n",
       "tweet_state_y                                                 -1.148103\n",
       "topics_cluster_7_x                                             0.241874\n",
       "topics_cluster_7_y                                            -2.937586\n",
       "tweet_region_x                                                 0.855007\n",
       "tweet_region_y                                                 0.781165\n",
       "tweet_category_x                                               -2.51108\n",
       "tweet_category_y                                              -1.380978\n",
       "user_gender_x                                                  2.260544\n",
       "user_gender_y                                                  4.517552\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prepped_data_url = 'https://www.dropbox.com/scl/fi/lw47ojiic0mzz9lp1xvl9/new_years_resolutions_prepped.parquet?rlkey=mc362g8s5x6bc3zqfhslf28oo&dl=1'\n",
    "\n",
    "df = pd.read_parquet(prepped_data_url)\n",
    "\n",
    "# have a little peek:\n",
    "print(f\"{df.shape}\")\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields explanations:\n",
    "* Up to `retweet_count` are the fields of the original base data, which can be [found here](https://github.com/aj-menon/Maven-Analytics/tree/master/2015_new_years_resolution_tweets), along with some further details. \n",
    "* The `_id` was added because some visualization tools require an integer unique id.\n",
    "* The `topics_cluster_7` was added by clustering the semantic embeddings of the `tweet_topics` text into 7 clusters (with the k-means method). This was done because `tweet_topics` alone has 115 unique values, which are a bit too many to use as a categorical. Seven is a more reasonable number for that. More on this in the data preparation section.\n",
    "* The `text` field contains the `tweet_topics` where we removed all hashtag words (# followed with alphanumericals and underscore)\n",
    "* All the `*_x` and `*_y` fields are simply planar projections of the `tweet_text` embeddings. All of these, except `tsne` and `umap`, are linear projections. That is, both x and y are obtained by linear transformation. So you can think of all these linear projections as different shadows of the 1500+ dimensional vector representation of the tweets onto different planes in that space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll look at different scatter plots of the planar projections of the (embeddings of the) tweets, using different fields to control the color of the points. \n",
    "When dealing with text data, the usual trick is to transform the text segments into fixed size numerical \"feature\" vectors. \n",
    "This is what most natural language processing techniques have been doing for years, with techniques like bag-of-words, tf-idf, word2vec, etc.\n",
    "Recently, the field has been revolutionized by the introduction of transformers, which are neural networks that can be trained to transform text into \n",
    "fixed size numerical vectors. Nowadays, in the new AI age sparked by `ChatGPT`, you can't throw a stone without hitting a transformer-based model that does \n",
    "some kind of text processing. \n",
    "\n",
    "Here, we used the OpenAI \"text-embedding-3-small\" embedding model to transform the text of the tweets into fixed size numerical vectors.\n",
    "This model will output a 1536-dimensional vector for each tweet.\n",
    "Now, how do we visualize these 1536-dimensional vectors?\n",
    "Well, we can't. We can't even **actually** visualize 3D vectors, let alone 1536D vectors. \n",
    "But the same way that we manage to display 3D vectors by projecting them onto the 2D plane of a compute screen can be used here. \n",
    "\n",
    "That said, there are many ways we can project 3D vectors onto a 2D plane.\n",
    "We'll go through a few of them here, also using different fields to control the color of the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using `cosmograph` for our scatter plot needs, since this will give us the ability to interact, share, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmograph import cosmo  # pip install cosmograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what fields have less than 200 unique values, to get a sense of what we might use to control the characteristics of our scatter plot points. \n",
    "(We might want to have no more than 10 unique values if we want to color by the value of that field.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_gender           2\n",
       "tweet_region          4\n",
       "topics_cluster_7      7\n",
       "tweet_category       10\n",
       "retweet_count        45\n",
       "user_timezone        50\n",
       "tweet_state          51\n",
       "tweet_topics        115\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df.nunique().sort_values()\n",
    "t[t < 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the `PCA` (principle component analysis) projection of our data. \n",
    "The two dimensions of the PCA projection are the two directions of the original data that capture the most variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca70edfac1c542c5863cccce7e4db52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by='pca_x',\n",
    "    point_y_by='pca_y',\n",
    "    point_color_by='tweet_category',\n",
    "    point_size_scale=0.003,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that this PCA projection manages to distinguish the humor and health categories somewhat,\n",
    "but the other categories are all over the place. This is not surprising, given that the PCA projection is\n",
    "a linear transformation maximizing variance. \n",
    "If we look at other fields like the region the tweet was sent from, or the declared gender of the tweeter (only 2 -- this is 2014!),\n",
    "we see that the PCA angle (first two components) doesn't offer a very informative view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809f9ca9c262465f962e12b70d13b7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by='pca_x',\n",
    "    point_y_by='pca_y',\n",
    "    point_color_by='tweet_region',\n",
    "    point_size_scale=0.003,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26ab601d7e84ec89ce84025e86e46fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by='pca_x',\n",
    "    point_y_by='pca_y',\n",
    "    point_color_by='user_gender',\n",
    "    point_size_scale=0.003,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at more powerful, non-linear, transformations. \n",
    "TSNE and UMAP are two classics in this matter. \n",
    "Essentially, the way they work is by trying to preserve the local structure of the data,\n",
    "which is very useful for visualization purposes, as it allows us to see clusters of data points that are similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71300d3ff7a748d2bf38d0e60156f392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by='tsne_x',\n",
    "    point_y_by='tsne_y',\n",
    "    point_color_by='tweet_category',\n",
    "    point_size_scale=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2602ee177ee45f29cb18ff67ae2b5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by='umap_x',\n",
    "    point_y_by='umap_y',\n",
    "    point_color_by='tweet_category',\n",
    "    point_size_scale=0.03,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separation of categories is definitely better with TSNE and UMAP than with PCA.\n",
    "One could even convince oneself that the tweets of a given region and gender tend to clump together slightly more than with PCA, though there is still a lot of overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4deec8773948d696f8cde9e55d1c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by='umap_x',\n",
    "    point_y_by='umap_y',\n",
    "    point_color_by='tweet_region',\n",
    "    point_size_scale=0.03,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a59fb467c24b719eb7bd77533c1af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by='tsne_x',\n",
    "    point_y_by='tsne_y',\n",
    "    point_color_by='user_gender',\n",
    "    point_size_scale=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4723"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But where we'll really start to get something exciting to look at is when we start to supervise our projections with the target variable\n",
    "we're trying to distinguish. \n",
    "Hold your horses, oh ye who know enough to critique, but not enough to be beyond blind dogma!\n",
    "1500 dimensions and only 5000 points? Aren't we overfitting? Aren't we just telling the projection what we want to see?\n",
    "Yes, we are. But I'm not going to humor that knee-jerk reaction with a response until we make it worse and get excited about the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at these beautiful tails for health & fitness, south of the figure, and finance stretching west on th figure. \n",
    "There's still a big pile of paella, but we've managed to really pull out some undeniable destinctions. \n",
    "\n",
    "[Explore this category angle further in the cosmograph app](https://cosmograph.fly.dev/public/408cafaa-59a6-4a6d-888a-0e2005e91e0e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108dc84d3cc24cee85211cd603704f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_column = 'tweet_category'\n",
    "\n",
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by=f'{category_column}_x',\n",
    "    point_y_by=f'{category_column}_y',\n",
    "    point_color_by=category_column,\n",
    "    point_size_scale=0.03,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's look at the hard ones now. \n",
    "\n",
    "See what `tweet_region` looks like now? Though the midwest is appropriately in the middle of everything, the south, northest, and west definitely have some tweet angles of their own!\n",
    "\n",
    "[Explore this region angle further in the cosmograph app](https://cosmograph.fly.dev/public/a9d47b81-31b4-4b3f-a23b-5132bcdc1682)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf45088af3d49c2919b33a374e06701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_column = 'tweet_region'\n",
    "\n",
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by=f'{category_column}_x',\n",
    "    point_y_by=f'{category_column}_y',\n",
    "    point_color_by=category_column,\n",
    "    point_size_scale=0.02,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more surprisingly (to me), even there's even an angle through which `gender` seems to affect the semantics of tweets. \n",
    "\n",
    "[Explore this gender angle further in the cosmograph app](https://cosmograph.fly.dev/public/a9af81f4-e5f5-44ff-ad9f-35c3c4113ed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b3657e1c5540e79a12c660495eba7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_column = 'user_gender'\n",
    "\n",
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by=f'{category_column}_x',\n",
    "    point_y_by=f'{category_column}_y',\n",
    "    point_color_by=category_column,\n",
    "    point_size_scale=0.07,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we even dare to have a peep at `tweet_state` (51 unique values) and `tweet_topic` (115 unique values)?\n",
    "\n",
    "Surely, we won't be able to get anything out of those, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c8e797e6334d66b60c93ee7e4ab816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_col = 'tweet_state'\n",
    "\n",
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by=f'{category_col}_x',\n",
    "    point_y_by=f'{category_col}_y',\n",
    "    point_color_by=category_col,\n",
    "    point_size_scale=0.03,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f03efb16de472599f7591ea52c7f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cosmograph(background_color=None, focused_point_ring_color=None, hovered_point_ring_color=None, link_color=Non…"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_col = 'tweet_topics'\n",
    "\n",
    "cosmo(\n",
    "    df,\n",
    "    point_label_by='tweet_text',\n",
    "    point_x_by=f'{category_col}_x',\n",
    "    point_y_by=f'{category_col}_y',\n",
    "    point_color_by=category_col,\n",
    "    point_size_scale=0.03,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_topics\n",
       "Other                                                    483\n",
       "Be more positive                                         463\n",
       "Improve my attitude                                      175\n",
       "Humor about Personal Growth and Interests Resolutions    155\n",
       "Humor about Health and Fitness Resolutions               155\n",
       "                                                        ... \n",
       "Work less                                                  3\n",
       "Start Master‰Ûªs program                                   3\n",
       "Start waking up earlier                                    2\n",
       "Fix up my home office                                      2\n",
       "Learn to Cook                                              1\n",
       "Name: count, Length: 115, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet_topics.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality: A Fair Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In high-dimensional spaces, overfitting looms large. With more dimensions than data points, it's dangerously easy to find projections that \"tell a story\" simply because they're tuned to the quirks of the data. Are we just finding angles that flatter our hypotheses?\n",
    "\n",
    "Surprisingly, the answer often leans toward \"no.\" Semantic embeddings aren't arbitrary - they're structured by design. The OpenAI model, like other transformer-based embeddings, organizes concepts into coherent clusters. Health resolutions, humor, and personal growth are not random blobs in this space - they're distinct regions.\n",
    "\n",
    "Supervised methods like LDA don't invent these structures; they amplify them. By aligning with categories already latent in the embeddings, they uncover patterns that might otherwise stay hidden. The key is validation - testing these projections on unseen data to ensure the stories we tell aren't mere mirages.\n",
    "\n",
    "That said, it’s important to remember that we’re not modeling here; we’re visualizing. While modeling and visualization are intricately connected, visualization has a different goal. It’s not primarily about predicting or estimating, so it’s less directly concerned with generalization. Instead, our objective is to offer multiple perspectives—different angles—on the data. By doing so, we empower the analyst to discover and explore diverse narratives, gaining a more complete understanding of the underlying realities the data reflects. Visualization is about inviting curiosity, sparking insights, and uncovering truths hidden in the multidimensional echo of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# USER: Set the save root directory here (this will be used to save data preparation artifacts)\n",
    "\n",
    "save_rootdir = '~/Dropbox/_odata/figiri/new_years_resolutions/'\n",
    "\n",
    "\n",
    "# assert that the directory exists\n",
    "save_rootdir = os.path.abspath(os.path.expanduser(save_rootdir))\n",
    "assert os.path.isdir(save_rootdir), f\"The directory save_rootdir you specified does not exist: {save_rootdir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openai_text_embeddings.parquet',\n",
       " 'with_hashtags/new_years_resolutions_prepped.parquet',\n",
       " 'with_hashtags/various_scatter_plots.pdf',\n",
       " 'openai_topics_embeddings.parquet',\n",
       " 'openai_tweet_text_embeddings.parquet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabled import DfFiles  # pip install tabled\n",
    "\n",
    "df_files = DfFiles(save_rootdir)\n",
    "list(df_files)  # list the files in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4723, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_created                                   2014-12-21 16:11:00\n",
       "tweet_text        #NewYearsResolution to not put the parking lot...\n",
       "tweet_category                                                Humor\n",
       "tweet_topics      Humor about Personal Growth and Interests Reso...\n",
       "tweet_location                                   City of Angels, CA\n",
       "tweet_state                                                      CA\n",
       "tweet_region                                                   West\n",
       "user_timezone                            Pacific Time (US & Canada)\n",
       "user_gender                                                    male\n",
       "retweet_count                                                   NaN\n",
       "_id                                                               0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_hashtags = True\n",
    "\n",
    "if with_hashtags:\n",
    "    prepped_data_file = 'with_hashtags/new_years_resolutions_prepped.parquet'\n",
    "    embeddings_file = 'openai_tweet_text_embeddings'  # 'openai_tweet_text_embeddings' or 'openai_text_embeddings.parquet'\n",
    "else:\n",
    "    prepped_data_file = 'without_hashtags/new_years_resolutions_prepped.parquet'\n",
    "    embeddings_file = 'openai_text_embeddings.parquet'\n",
    "\n",
    "if prepped_data_file not in df_files:\n",
    "    original_src_url = 'https://raw.githubusercontent.com/aj-menon/Maven-Analytics/refs/heads/master/2015_new_years_resolution_tweets/New_years_resolutions.csv'\n",
    "    df = pd.read_csv(original_src_url)\n",
    "    df_files[prepped_data_file] = df\n",
    "    df['_id'] = df.index.values  # because some plotting libraries need a unique id as a column\n",
    "else:\n",
    "    df = df_files[prepped_data_file]\n",
    "\n",
    "print(f\"{df.shape}\")\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poke around the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4723"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet_text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data spans from 2014-12-21 16:11:00 to 2015-01-02 09:54:00\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data spans from {df.tweet_created.min()} to {df.tweet_created.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What datas are missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_timezone    3496\n",
       "retweet_count    2932\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df.notna().sum()\n",
    "t[t!=len(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791 missing retweet counts\n",
      "2215 zero retweet counts\n",
      "717 non-zero retweet counts\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df.retweet_count.isna().sum()} missing retweet counts\")\n",
    "print(f\"{(df.retweet_count == 0).sum()} zero retweet counts\")\n",
    "print(f\"{(df.retweet_count > 0).sum()} non-zero retweet counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What datas are categorical (or can be used as such in visualizations)?\n",
    "\n",
    "First let's look at the number of unique values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_created     2738\n",
       "tweet_text        4723\n",
       "tweet_category      10\n",
       "tweet_topics       115\n",
       "tweet_location    2630\n",
       "tweet_state         51\n",
       "tweet_region         4\n",
       "user_timezone       50\n",
       "user_gender          2\n",
       "retweet_count       45\n",
       "_id               4723\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_category    10\n",
       "tweet_region       4\n",
       "user_gender        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_uniques = 10\n",
    "df.nunique()[df.nunique() <= max_uniques]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3748"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many (lower cased) tweet_text strings contain the subsetring \"#newyearsresolution\"?\n",
    "\n",
    "df.tweet_text.str.lower().str.contains('#newyearsresolution').sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute embeddings of the tweet texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get clean text (remove hashtag words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "hashtag_pattern = re.compile(r\"#\\w+\")\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return hashtag_pattern.sub('', text).strip()\n",
    "\n",
    "df['text'] = df['tweet_text'].apply(remove_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = 'openai_tweet_text_embeddings.parquet'\n",
    "\n",
    "if embeddings_file not in df_files:\n",
    "    from oa import embeddings\n",
    "\n",
    "    vectors = embeddings(df.tweet_text)  # ~14s\n",
    "    vectors_df = pd.DataFrame(data=vectors, index=df.index.values)\n",
    "    df_files[embeddings_file] = vectors_df\n",
    "else:\n",
    "    vectors_df = df_files[embeddings_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = 'openai_text_embeddings.parquet'\n",
    "\n",
    "if embeddings_file not in df_files:\n",
    "    from oa import embeddings\n",
    "\n",
    "    vectors = embeddings(df.text)  # ~14s\n",
    "    vectors_df = pd.DataFrame(data=vectors, index=df.index.values)\n",
    "    df_files[embeddings_file] = vectors_df\n",
    "else:\n",
    "    vectors_df = df_files[embeddings_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = df.tweet_topics.unique()\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_save_file = 'openai_topics_embeddings.parquet'\n",
    "\n",
    "if topics_save_file not in df_files:\n",
    "    from oa import embeddings\n",
    "\n",
    "    topics_vectors = embeddings(topics)  # ~14s\n",
    "    topics_vectors_df = pd.DataFrame(data=topics_vectors, index=topics)\n",
    "    df_files[topics_save_file] = topics_vectors_df\n",
    "else:\n",
    "    topics_vectors_df = df_files[topics_save_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute topics (embeddings) clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics_cluster_7\n",
       "4    848\n",
       "2    847\n",
       "6    773\n",
       "3    754\n",
       "0    665\n",
       "1    516\n",
       "5    320\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "new_col = 'topics_cluster_7'\n",
    "\n",
    "if new_col not in df.columns:\n",
    "    kmeans = KMeans(n_clusters=7)\n",
    "    cluster_indices = kmeans.fit_predict(topics_vectors_df.values)\n",
    "    clusters_df = pd.DataFrame(\n",
    "        data=cluster_indices, \n",
    "        index=topics_vectors_df.index, \n",
    "        columns=[new_col]\n",
    "    )\n",
    "    # join df to clusters_df, left on tweet_topics, right on index\n",
    "    df = df.join(clusters_df, on='tweet_topics')\n",
    "    df_files[prepped_data_file] = df\n",
    "\n",
    "df.topics_cluster_7.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute planar projections of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised projection: pca, tsne and umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = 'openai_tweet_text_embeddings'  # 'openai_tweet_text_embeddings' or 'openai_text_embeddings.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_created                                   2014-12-21 16:11:00\n",
       "tweet_text        #NewYearsResolution to not put the parking lot...\n",
       "tweet_category                                                Humor\n",
       "tweet_topics      Humor about Personal Growth and Interests Reso...\n",
       "tweet_location                                   City of Angels, CA\n",
       "tweet_state                                                      CA\n",
       "tweet_region                                                   West\n",
       "user_timezone                            Pacific Time (US & Canada)\n",
       "user_gender                                                    male\n",
       "retweet_count                                                   NaN\n",
       "_id                                                               0\n",
       "text              to not put the parking lot ticket directly in ...\n",
       "pca_x                                                      0.196627\n",
       "pca_y                                                     -0.210784\n",
       "tsne_x                                                      7.15576\n",
       "tsne_y                                                     5.774322\n",
       "umap_x                                                     0.199604\n",
       "umap_y                                                     0.709486\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes ~50s\n",
    "from imbed import planar_embeddings, planar_embeddings_dict_to_df\n",
    "\n",
    "vectors_df = df_files[embeddings_file]\n",
    "\n",
    "if 'pca_x' not in df.columns:\n",
    "\n",
    "    pca_xy = planar_embeddings(vectors_df.values, embeddings_func='pca')\n",
    "    pca_xy = planar_embeddings_dict_to_df(pca_xy, x_col='pca_x', y_col='pca_y')\n",
    "\n",
    "    df = pd.concat([df, pca_xy], axis=1)\n",
    "    df_files[prepped_data_file] = df\n",
    "\n",
    "if 'tsne_x' not in df.columns:\n",
    "\n",
    "    tsne_xy = planar_embeddings(vectors_df.values, embeddings_func='tsne')\n",
    "    tsne_xy = planar_embeddings_dict_to_df(tsne_xy, x_col='tsne_x', y_col='tsne_y')\n",
    "\n",
    "    df = pd.concat([df, tsne_xy], axis=1)\n",
    "    df_files[prepped_data_file] = df\n",
    "\n",
    "if 'umap_x' not in df.columns:\n",
    "\n",
    "    umap_xy = planar_embeddings(vectors_df.values, embeddings_func='umap')\n",
    "    umap_xy = planar_embeddings_dict_to_df(umap_xy, x_col='umap_x', y_col='umap_y')\n",
    "\n",
    "    df = pd.concat([df, umap_xy], axis=1)\n",
    "    df_files[prepped_data_file] = df\n",
    "\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (supervised )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_category     10\n",
       "tweet_topics      115\n",
       "tweet_state        51\n",
       "tweet_region        4\n",
       "user_timezone      50\n",
       "user_gender         2\n",
       "retweet_count      45\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what columns have less than 200 unique values (canditates for categorical columns)\n",
    "t = df.nunique()\n",
    "t[t < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lda_cols(df, vectors_df, category_col):\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "    lda = LDA(n_components=2)\n",
    "    lda_xy = lda.fit_transform(vectors_df.values, df[category_col].values)\n",
    "    lda_xy = pd.DataFrame(data=lda_xy, columns=[f'{category_col}_x', f'{category_col}_y'])\n",
    "    df[[f'{category_col}_x', f'{category_col}_y']] = lda_xy\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'topics_cluster_7_x' not in df.columns:    \n",
    "    df = add_lda_cols(df, vectors_df, category_col='tweet_topics')\n",
    "    df = add_lda_cols(df, vectors_df, category_col='tweet_state')\n",
    "    df = add_lda_cols(df, vectors_df, category_col='tweet_region')\n",
    "    df = add_lda_cols(df, vectors_df, category_col='tweet_category')\n",
    "    df = add_lda_cols(df, vectors_df, category_col='topics_cluster_7')\n",
    "\n",
    "    df_files[prepped_data_file] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the `user_gender`? Why didn’t we get an `LDA` projection for that? \n",
    "Because `user_gender` has only two possible values in this data, and _Linear Discriminant Analysis (LDA)_ can only produce projections with dimensions one less than the number of classes. This means it cannot create a 2D projection for binary categories. \n",
    "\n",
    "_Partial Least Squares (PLS)_, on the other hand, has no such limitation. PLS works by finding linear combinations of features that maximize the covariance between the input data and the target variable, allowing it to produce projections of any desired dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "category_col = 'user_gender'\n",
    "X, y = vectors_df.values, df[category_col].values\n",
    "\n",
    "yy = pd.Categorical(y).codes  # ensure we have integer labels (PLS requires this)\n",
    "yy = yy.reshape(-1, 1)  # Convert y to a 2D array (PLS requires y as a 2D array)\n",
    "pls_xy = PLSRegression(n_components=2).fit(X, yy).transform(X)\n",
    "pls_xy = pd.DataFrame(data=pls_xy, columns=[f'{category_col}_x', f'{category_col}_y'])\n",
    "df[[f'{category_col}_x', f'{category_col}_y']] = pls_xy\n",
    "\n",
    "df_files[prepped_data_file] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = [\n",
    "    'tweet_created',\n",
    "    'tweet_text',\n",
    "    'tweet_category',\n",
    "    'tweet_topics',\n",
    "    'tweet_location',\n",
    "    'tweet_state',\n",
    "    'tweet_region',\n",
    "    'user_timezone',\n",
    "    'user_gender',\n",
    "    'retweet_count',\n",
    "    'text',\n",
    "    'topics_cluster_7',\n",
    "    '_id',\n",
    "    'pca_x',\n",
    "    'pca_y',\n",
    "    'tsne_x',\n",
    "    'tsne_y',\n",
    "    'umap_x',\n",
    "    'umap_y',\n",
    "    'tweet_topics_x',\n",
    "    'tweet_topics_y',\n",
    "    'tweet_state_x',\n",
    "    'tweet_state_y',\n",
    "    'topics_cluster_7_x',\n",
    "    'topics_cluster_7_y',\n",
    "    'tweet_region_x',\n",
    "    'tweet_region_y',\n",
    "    'tweet_category_x',\n",
    "    'tweet_category_y',\n",
    "    'user_gender_x',\n",
    "    'user_gender_y',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[column_order]\n",
    "df_files[prepped_data_file] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try multiple x/y/color_by combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use seaborn to make a bunch of scatter plots with different combinations of projections and color field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('pca_x', 'pca_y', 'tweet_category'),\n",
       " ('pca_x', 'pca_y', 'tweet_state'),\n",
       " ('pca_x', 'pca_y', 'tweet_region'),\n",
       " ('pca_x', 'pca_y', 'user_gender'),\n",
       " ('pca_x', 'pca_y', 'topics_cluster_7'),\n",
       " ('pca_x', 'pca_y', 'tweet_topics'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_fields = [\n",
    "    ('pca_x', 'pca_y'),\n",
    "    ('tsne_x', 'tsne_y'),\n",
    "    ('umap_x', 'umap_y'),\n",
    "]\n",
    "category_cols = [\n",
    "    'tweet_category',\n",
    "    'tweet_region', \n",
    "    'user_gender',\n",
    "    'topics_cluster_7', \n",
    "    'tweet_topics', \n",
    "    'tweet_state', \n",
    "]\n",
    "supervised_combinations = [\n",
    "    ('tweet_category_x', 'tweet_category_y', 'tweet_category'),\n",
    "    ('tweet_region_x', 'tweet_region_y', 'tweet_region'),\n",
    "    ('user_gender_x', 'user_gender_y', 'user_gender'),\n",
    "    ('topics_cluster_7_x', 'topics_cluster_7_y', 'topics_cluster_7'),\n",
    "    ('tweet_topics_x', 'tweet_topics_y', 'tweet_topics'),\n",
    "    ('tweet_state_x', 'tweet_state_y', 'tweet_state'),\n",
    "]\n",
    "\n",
    "import itertools\n",
    "from lkj import chunker\n",
    "\n",
    "xy_category_fields = [tuple([*xy, category]) for xy, category in itertools.product(xy_fields, category_cols)]\n",
    "xy_category_fields = list(xy_category_fields) + supervised_combinations\n",
    "\n",
    "batch = next(chunker(xy_category_fields, 6))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a 3x2 grid of plots, plotting each of the batch combinations as a scatter plot\n",
    "# where each of the triples of the batch is (x_col, y_col, color_col)\n",
    "# only use the first two categories of the color in the legend, and use elipses for the rest\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_32(batch, include_legend=False):\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 18))\n",
    "    for i, (x_col, y_col, color_col) in enumerate(batch):\n",
    "        ax = axs[i // 2, i % 2]\n",
    "        sns.scatterplot(data=df, x=x_col, y=y_col, hue=color_col, ax=ax, alpha=0.5)\n",
    "        ax.set_title(f'{x_col[:-2]}\\ncolored by {color_col}')\n",
    "        if not include_legend:\n",
    "            ax.get_legend().remove()\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    return fig\n",
    "\n",
    "def pdf_bytes_of_batches():\n",
    "    import io\n",
    "    for batch in chunker(xy_category_fields, 6):\n",
    "        fig = scatter_32(batch)\n",
    "        pdf_bytes = io.BytesIO()\n",
    "        fig.savefig(pdf_bytes, format='pdf')\n",
    "        plt.close(fig)  # Close the current figure to suppress display\n",
    "        yield pdf_bytes.getvalue()\n",
    "\n",
    "from pdfdol import concat_pdfs\n",
    "\n",
    "combined_pdf_bytes = concat_pdfs(pdf_bytes_of_batches())\n",
    "\n",
    "# save these pdf bytes to a file in the save_rootdir\n",
    "from dol import Files \n",
    "\n",
    "project_files = Files(save_rootdir)\n",
    "project_files['various_scatter_plots.pdf'] = combined_pdf_bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a 2x3 grid of plots, plotting each of the batch combinations as a scatter plot\n",
    "# where each of the triples of the batch is (x_col, y_col, color_col)\n",
    "# only use the first two categories of the color in the legend, and use elipses for the rest\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_23(batch, include_legend=False):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 12))  # Updated to 2 rows and 3 columns\n",
    "    for i, (x_col, y_col, color_col) in enumerate(batch):\n",
    "        ax = axs[i // 3, i % 3]  # Adjust indexing for a 2x3 grid\n",
    "        sns.scatterplot(data=df, x=x_col, y=y_col, hue=color_col, ax=ax, alpha=0.5)\n",
    "        ax.set_title(f'{x_col[:-2]}\\ncolored by {color_col}')\n",
    "        if not include_legend:\n",
    "            ax.get_legend().remove()\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    return fig\n",
    "\n",
    "def pdf_bytes_of_batches():\n",
    "    import io\n",
    "    for batch in chunker(xy_category_fields, 6):\n",
    "        fig = scatter_23(batch)\n",
    "        pdf_bytes = io.BytesIO()\n",
    "        fig.savefig(pdf_bytes, format='pdf')\n",
    "        plt.close(fig)  # Close the current figure to suppress display\n",
    "        yield pdf_bytes.getvalue()\n",
    "\n",
    "from pdfdol import concat_pdfs\n",
    "\n",
    "combined_pdf_bytes = concat_pdfs(pdf_bytes_of_batches())\n",
    "\n",
    "# save these pdf bytes to a file in the save_rootdir\n",
    "from dol import Files \n",
    "\n",
    "project_files = Files(save_rootdir)\n",
    "project_files['various_scatter_plots_23.pdf'] = combined_pdf_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
